"""
FM Korea ê²Œì‹œë¬¼ ìˆ˜ì§‘ ëª¨ë“ˆ
íšŒì›ë²ˆí˜¸ë¡œ ê²€ìƒ‰ ë˜ëŠ” ì§ì ‘ URLë¡œ ê²Œì‹œë¬¼ ìˆ˜ì§‘
"""

import asyncio
import json
import sys
from typing import List, Dict, Callable, Optional
from playwright.async_api import Page
from .browser import create_stealth_browser, create_context, handle_cloudflare_challenge, random_delay
from .parser import parse_post_html


async def collect_posts_by_member(
    member_id: str,
    max_pages: int = 10,
    progress_callback: Optional[Callable] = None
) -> List[str]:
    """
    íšŒì›ë²ˆí˜¸ë¡œ ê²Œì‹œë¬¼ URL ëª©ë¡ ìˆ˜ì§‘
    
    Args:
        member_id: FM Korea íšŒì›ë²ˆí˜¸
        max_pages: ìµœëŒ€ í˜ì´ì§€ ìˆ˜
        progress_callback: ì§„í–‰ë¥  ì½œë°± í•¨ìˆ˜
    
    Returns:
        ê²Œì‹œë¬¼ URL ë¦¬ìŠ¤íŠ¸
    """
    browser = await create_stealth_browser(headless=False)
    context = await create_context(browser)
    page = await context.new_page()
    
    post_urls = []
    
    try:
        for page_num in range(1, max_pages + 1):
            search_url = f"https://www.fmkorea.com/search.php?mid=stock&search_target=member_srl&search_keyword={member_id}&page={page_num}"
            
            if progress_callback:
                progress_callback(f"í˜ì´ì§€ {page_num}/{max_pages} ë¡œë”© ì¤‘...", page_num / max_pages * 50)
            
            print(f"ğŸ“„ í˜ì´ì§€ {page_num} ì ‘ê·¼ ì¤‘: {search_url}")
            
            await page.goto(search_url, wait_until="domcontentloaded")
            await random_delay(3, 5)
            
            # Cloudflare ì±Œë¦°ì§€ ì²˜ë¦¬
            await handle_cloudflare_challenge(page)
            
            # ê²Œì‹œë¬¼ ë§í¬ ì¶”ì¶œ
            links = await page.locator('a.hx').all()
            
            if not links:
                print(f"âš ï¸  í˜ì´ì§€ {page_num}ì—ì„œ ê²Œì‹œë¬¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²€ìƒ‰ ì¢…ë£Œ.")
                break
            
            for link in links:
                href = await link.get_attribute('href')
                if href and '/board/' not in href:  # ëŒ“ê¸€ ë§í¬ ì œì™¸
                    full_url = f"https://www.fmkorea.com{href}" if href.startswith('/') else href
                    if full_url not in post_urls:
                        post_urls.append(full_url)
            
            print(f"âœ… í˜ì´ì§€ {page_num}: {len(links)}ê°œ ê²Œì‹œë¬¼ ë°œê²¬")
            
            await random_delay(2, 4)
        
        print(f"\nğŸ¯ ì´ {len(post_urls)}ê°œ ê²Œì‹œë¬¼ URL ìˆ˜ì§‘ ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ ì—ëŸ¬ ë°œìƒ: {e}")
    finally:
        try:
            await browser.close()
        except:
            pass
    
    return post_urls


async def collect_posts(
    urls: List[str],
    output_dir: str = "data/raw",
    progress_callback: Optional[Callable] = None
) -> List[str]:
    """
    ê²Œì‹œë¬¼ URL ë¦¬ìŠ¤íŠ¸ì—ì„œ ìƒì„¸ ë‚´ìš© ìˆ˜ì§‘ (ê°œë³„ íŒŒì¼ë¡œ ì¦‰ì‹œ ì €ì¥)
    
    Args:
        urls: ê²Œì‹œë¬¼ URL ë¦¬ìŠ¤íŠ¸
        output_dir: ì €ì¥ ë””ë ‰í† ë¦¬
        progress_callback: ì§„í–‰ë¥  ì½œë°± í•¨ìˆ˜
    
    Returns:
        ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
    """
    from pathlib import Path
    import hashlib
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    browser = await create_stealth_browser(headless=False)
    context = await create_context(browser)
    page = await context.new_page()
    
    saved_files = []
    total = len(urls)
    
    try:
        for idx, url in enumerate(urls, 1):
            if progress_callback:
                progress_callback(f"ê²Œì‹œë¬¼ {idx}/{total} ìˆ˜ì§‘ ì¤‘...", 50 + (idx / total * 50))
            
            print(f"\nğŸ“ [{idx}/{total}] {url}")
            
            try:
                await page.goto(url, wait_until="domcontentloaded", timeout=30000)
                await random_delay(2, 4)
                
                # HTML ê°€ì ¸ì˜¤ê¸°
                html = await page.content()
                
                # íŒŒì‹±
                post_data = parse_post_html(html, url)
                
                if post_data:
                    # URL í•´ì‹œë¡œ íŒŒì¼ëª… ìƒì„± (ì¤‘ë³µ ë°©ì§€)
                    url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
                    filename = f"post_{url_hash}.json"
                    filepath = output_path / filename
                    
                    # ì¦‰ì‹œ íŒŒì¼ë¡œ ì €ì¥ (ë©”ëª¨ë¦¬ ì ˆì•½)
                    with open(filepath, 'w', encoding='utf-8') as f:
                        json.dump(post_data, f, ensure_ascii=False, indent=2)
                    
                    saved_files.append(str(filepath))
                    print(f"âœ… ì €ì¥: {filename} - {post_data.get('title', 'N/A')[:50]}...")
                else:
                    print(f"âš ï¸  íŒŒì‹± ì‹¤íŒ¨")
                
            except Exception as e:
                print(f"âŒ ì—ëŸ¬: {e}")
                continue
        
        print(f"\nğŸ‰ ì´ {len(saved_files)}ê°œ ê²Œì‹œë¬¼ íŒŒì¼ ì €ì¥ ì™„ë£Œ")
        print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {output_path.absolute()}")
        
    except Exception as e:
        print(f"âŒ ì „ì²´ ì—ëŸ¬: {e}")
    finally:
        try:
            await browser.close()
        except:
            pass
    
    return saved_files


async def extract_post_data(page: Page) -> Dict:
    """
    í˜„ì¬ í˜ì´ì§€ì—ì„œ ê²Œì‹œë¬¼ ë°ì´í„° ì¶”ì¶œ
    
    Args:
        page: Playwright Page ì¸ìŠ¤í„´ìŠ¤
    
    Returns:
        ê²Œì‹œë¬¼ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
    """
    html = await page.content()
    url = page.url
    return parse_post_html(html, url)
